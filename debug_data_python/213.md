### 问题描述：
<p>线性回归，回答正确加酬金（我开设了几个问题）</p>
问题遇到的现象和发生背景
输出空值，线性回归计算
问题相关代码，请勿粘贴截图

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt


img = cv2.imread(r'C:\Users\Xpc\Desktop\weixin2222 change40.jpg')
hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
low_hsv = np.array([0, 0, 221])
high_hsv = np.array([180, 30, 255])
mask = cv2.inRange(hsv, lowerb=low_hsv, upperb=high_hsv)


list_y = []
list_x = []

for i in range(len(mask)):
    xmax = []
    for j in range(len(mask[i])):
        if mask[i][j] == 0:
            list_x.append(j)
            list_y.append(len(mask)-i)

plt.plot(list_x, list_y, 'o', color='r')
plt.show()

x_array=np.array(list_x)
x_array=x_array/400
y_array=np.array(list_y)
y_array=y_array*0.2/400

z40=np.stack((x_array,y_array),axis=0).T
z0=[]
for i in range(0,z40.shape[0],30):
    z1=z40[i,:]
    z0.append(z1)
z0=np.array(z0)

x_array=z0[:,0]
y_array=z0[:,1]
print(x_array)
print(y_array)

list_x1=[]
for i in range(0,len(x_array)):
    list_x1.append(40)
x1_array=np.array(list_x1)

list_x0=[]
for i in range(0,len(x_array)):
    list_x0.append(1)
x0=np.array(list_x0)
x1=x_array
x2=x1_array
x3=x1*x2
x4=x1*x1
x5=x2*x2
label40=y_array



z40=np.stack((x0,x1,x2,x3,x4,x5),axis=0).T

rescombine = z40
labels= y_array
data=rescombine

np.seterr(divide='ignore',invalid='ignore')
class LinearRegression:
    def __init__(self,data,labels):
        self.data = data
        self.labels = labels
        num_features = len(data[0])
        self.theta = np.zeros((num_features,1))
        


    def train(self,alpha,num_iterations = 500):
       
        cost_history = self.gradient_descent(alpha,num_iterations)
        return self.theta,cost_history
        
    def gradient_descent(self,alpha,num_iterations):
        cost_history= []
        for _ in range(num_iterations):
            self.gradient_step(alpha)
            cost_history.append(self.cost_function(self.data,self.labels))
        return cost_history
        
        
    def gradient_step(self,alpha):    
        num_examples = data.shape[0]
        prediction = LinearRegression.hypothesis(self.data,self.theta)
        delta = prediction - self.labels
        theta = self.theta
        theta = theta - alpha*(1/num_examples)*(np.dot(delta.T,self.data)).T
        self.theta = theta
        
        
    def cost_function(self,data,labels):
        self.m = len(labels) 
        delta = LinearRegression.hypothesis(data,self.theta) - labels
        cost = (1/2)*np.dot(delta.T,delta)/self.m
        return cost[0][0]
        
    
    def hypothesis(data,theta):   
        predictions = np.dot(data,theta)
        return predictions
        

x_train = rescombine

y_train = labels

num_iterations = 500
learning_rate = 0.01  


linear_regression = LinearRegression(x_train, y_train)

(theta, cost_history) = linear_regression.train(learning_rate, num_iterations)



print (theta, cost_history)
print(len( cost_history))



```

运行结果及报错内容
[0.3025 0.315  0.3275 0.3425 0.36   0.365  0.3775 0.395  0.41   0.4325 0.4475 0.46   0.4825 0.4925 0.5075 0.52   0.5375 0.5625 0.995  0.5825 0.9825 0.6    0.945  0.95   0.955  0.9525 0.63   0.6475 0.6475 0.655 0.8975 0.86   0.8125 0.685  0.805  0.71   0.785  0.7475 0.8225][0.1365 0.1305 0.125  0.121  0.1175 0.114  0.111  0.1065 0.103  0.0985 0.0945 0.092  0.0895 0.087  0.0845 0.082  0.0795 0.0775 0.0755 0.074 0.0735 0.0725 0.072  0.0715 0.071  0.0705 0.0695 0.069  0.0685 0.068 0.068  0.0675 0.067  0.0665 0.0665 0.066  0.066  0.0655 0.0655][[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan  nan nan nan]] [6115546.293053937, 4014534661896851.5, 2.635330971147491e+24, 1.729956249576308e+33, 1.1356253382264945e+42, 7.454783374654096e+50, 4.893673405509658e+59, 3.2124393421296037e+68, 2.1087975579333355e+77, 1.3843147423906104e+86, 9.087298582980243e+94, 5.965333822395535e+103, 3.915928071216269e+112, 2.5706009278759203e+121, 1.687464378870506e+130, 1.1077316588031153e+139, 7.27167603227295e+147, 4.7734730607470265e+156, 3.1335341344346484e+165, 2.0570004369377248e+174, 1.3503126553064964e+183, 8.864092755348704e+191, 5.818810929946509e+200, 3.819743494677929e+209, 2.5074608095693e+218, 1.6460162103256778e+227, 1.080523114983515e+236, 7.093066244971383e+244, 4.656225124468534e+253, 3.0565670277085482e+262, 2.006475577346884e+271, 1.3171457409549063e+280, 8.646369397676056e+288, 5.675886990825303e+297, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]500
我的解答思路和尝试过的方法
很懵
我想要达到的结果
正常运算 
### 修改方案：
我没按你的来，按自己的理解实现了一下，然后分别试了1000、10000、100000轮的效果

```python
import numpy as np
import matplotlib.pyplot as plt
import cv2

class LinearRegression:
    def __init__(self,data,labels):
        self.data = data
        self.labels = labels
        self.features = np.zeros((1,self.data.shape[1]))
    
    def train(self,learning_rate, num_iterations):
        for i in range(num_iterations):
            self.step_gradient(learning_rate)
            # loss = self.loss_fuction()
            # print(f'第{i}轮loss={loss}, features={self.features}')
        return self.features
    
    def step_gradient(self, learning_rate):
        N = float(len(self.labels))
        err_current = np.sum(self.features*self.data,axis=1) - self.labels
        features_gradient = np.array([sum([x**i*err for x,err in zip(self.data[:,i],err_current)])*(2/N) for i in range(self.features.shape[1])])
        self.features = self.features - (learning_rate* features_gradient)
    
    def loss_fuction(self):
        totalError = sum([(y-(np.sum(self.features*x,axis=1)))**2 for x,y in zip(self.data,self.labels)])
        return totalError / float(len(self.labels))
    
    def draw(self,num_iterations):
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        x = self.data[:,1]
        sl = [s[0] for s in sorted(enumerate(x), key=lambda a:a[1])]
        y = [sum(x*self.features[0]) for x in self.data]
        sort_y = [y[i] for i in sl]
        sort_labels = [self.labels[i] for i in sl]
        x.sort()
        plt.scatter(x,sort_labels,label='source')
        plt.plot(x, sort_y,color='r',label='predict')
        plt.xlabel('X')
        plt.ylabel('Y')
        plt.title(str(num_iterations)+'轮')
        plt.legend()
        plt.show()
 
if __name__ == '__main__':
    img = cv2.imread('line.jpg')
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    low_hsv = np.array([0, 0, 221])
    high_hsv = np.array([180, 30, 255])
    mask = cv2.inRange(hsv, lowerb=low_hsv, upperb=high_hsv)
    list_y = []
    list_x = []
    
    for i in range(0,len(mask),5):
        xmax = []
        for j in range(0,len(mask[i]),5):
            if mask[i][j] == 0:
                list_x.append(j)
                list_y.append(i)
    list_x = [x/100 for x in list_x]
    list_y = [y/100 for y in list_y]
    list_x = [[1,x,x**2] for x in list_x]
    learning_rate = 0.001
    num_iterations = 1000
    lr = LinearRegression(np.array(list_x),np.array(list_y))
    lr.train(learning_rate,num_iterations)
    lr.draw(num_iterations)

```


### 人工打分：
